{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ejercicio1-Paralela.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMP0VLZqkt60"
      },
      "source": [
        "#**1 Introduccion**\n",
        "En el siguiente cuaderno se realiza el codigo para invertir un vector[1], es decir:\n",
        "<center>$Dado A=(a1,a2,a3,a4,a5)$</center>\n",
        "<center>$El resultado es A´(a5,a4,a3,a2,a1)$</center>\n",
        "\n",
        "Para el cual se pide por parametro la cantidad de elementos para vector a invertir. \n",
        "\n",
        "En cuanto a la paralelizacion[2,3]:\n",
        "\n",
        "Se define la memoria de los vectores en CPU, se debe reeservar espacio de memoria GPU para los vectores a usar y luego se debe realizar la transferencia de datos de CPU a GPU. Luego, se debe definir la función kernel que ejecutará en GPU, dentro de esta funcion, para acceder al indice de la malla y al indice del hilo dentro del bloque se debe emplear: **blockIdx.x** y **threadIdx.x**;\n",
        "Luego se calcula los índices globales donde cada hilo tenga que trabajar\n",
        "en un área de datos diferente según la partición:\n",
        "**int Idx = blockIdx.x * blockDim.x + threadIdx.x;** \n",
        "Genero la funcion *kernel* y luego lo ejecuto.\n",
        "Para poder mostrar por pantalla el resultado, realizo la transferencia de dados de GPU a CPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8LGHi76mFT9"
      },
      "source": [
        "#**2 Armado del Ambiente**\n",
        "Instala en el cuaderno el modulo CUDA de Python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etJu7Z3OmZXv",
        "outputId": "9f987722-74e5-47d7-e336-2aaf894a7d04"
      },
      "source": [
        "!pip install pycuda"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pycuda\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/61/47d3235a4c13eec5a5f03594ddb268f4858734e02980afbcd806e6242fa5/pycuda-2020.1.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 7.0MB/s \n",
            "\u001b[?25hCollecting pytools>=2011.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b7/30/c9362a282ef89106768cba9d884f4b2e4f5dc6881d0c19b478d2a710b82b/pytools-2020.4.3.tar.gz (62kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: decorator>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from pycuda) (4.4.2)\n",
            "Collecting appdirs>=1.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/00/2344469e2084fb287c2e0b57b72910309874c3245463acd6cf5e3db69324/appdirs-1.4.4-py2.py3-none-any.whl\n",
            "Collecting mako\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/37/0e706200d22172eb8fa17d68a7ae22dec7631a0a92266634fb518a88a5b2/Mako-1.1.3-py2.py3-none-any.whl (75kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 11.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.8.0 in /usr/local/lib/python3.6/dist-packages (from pytools>=2011.2->pycuda) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from pytools>=2011.2->pycuda) (1.18.5)\n",
            "Requirement already satisfied: dataclasses>=0.7 in /usr/local/lib/python3.6/dist-packages (from pytools>=2011.2->pycuda) (0.7)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from mako->pycuda) (1.1.1)\n",
            "Building wheels for collected packages: pycuda, pytools\n",
            "  Building wheel for pycuda (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycuda: filename=pycuda-2020.1-cp36-cp36m-linux_x86_64.whl size=621008 sha256=bad6e4180f9e6ebd90d4ade9433dde61d43a29b2864bf01db556455ec2742924\n",
            "  Stored in directory: /root/.cache/pip/wheels/8f/78/d1/5bb826f81d9d490297a348d818ff3ee6dd6f2075b06dde6ea0\n",
            "  Building wheel for pytools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytools: filename=pytools-2020.4.3-py2.py3-none-any.whl size=61374 sha256=23bc5be808066b2b218c70acd7134c846121f97009be5bcf8c48aaf63ac74d6b\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/c7/81/a22edb90b0b09a880468b2253bb1df8e9f503337ee15432c64\n",
            "Successfully built pycuda pytools\n",
            "Installing collected packages: appdirs, pytools, mako, pycuda\n",
            "Successfully installed appdirs-1.4.4 mako-1.1.3 pycuda-2020.1 pytools-2020.4.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1Zst-iAmRBd"
      },
      "source": [
        "#**3 Desarrollo**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdFtkGtBmbO1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e39365a9-c379-4ded-811c-9cb1f65393cf"
      },
      "source": [
        "# --------------------------------------------\n",
        "#@title 3.1 Parámetros de ejecución { vertical-output: true }\n",
        "\n",
        "cant =   5#@param {type: \"number\"}\n",
        "\n",
        "# --------------------------------------------\n",
        "from datetime import datetime\n",
        "\n",
        "tiempo_total = datetime.now()\n",
        "\n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "from pycuda.compiler import SourceModule\n",
        "\n",
        "import numpy\n",
        "\n",
        "try:\n",
        "\n",
        "  tiempo_en_ms = lambda dt:(dt.days * 24 * 60 * 60 + dt.seconds) * 1000 + dt.microseconds / 1000.0\n",
        "  \n",
        "  vec1_cpu = numpy.random.randn( cant )\n",
        "  vec1_cpu = vec1_cpu.astype( numpy.int32() )\n",
        "\n",
        "  vec2_cpu = numpy.random.randn(cant)\n",
        "  vec2_cpu = vec2_cpu.astype(numpy.int32())\n",
        "\n",
        "  vec1_gpu = cuda.mem_alloc( vec1_cpu.nbytes )\n",
        "  vec2_gpu = cuda.mem_alloc( vec2_cpu.nbytes )\n",
        "\n",
        "  cuda.memcpy_htod( vec1_gpu, vec1_cpu )\n",
        "  cuda.memcpy_htod( vec2_gpu, vec2_cpu )\n",
        "\n",
        "  print(\"vector\", vec1_cpu)\n",
        "\n",
        "  module = SourceModule(\"\"\"\n",
        "  __global__ void invertirVector( int n, int *V, int *Y)\n",
        "  {\n",
        "    int idx = threadIdx.x + blockIdx.x*blockDim.x;\n",
        "  \n",
        "    if( idx <= n ){\n",
        "      Y[idx] = V[n-1-idx];\n",
        "    }\n",
        "  }\n",
        "  \"\"\") \n",
        "  # CPU - Genero la función kernel.\n",
        "  kernel = module.get_function(\"invertirVector\")\n",
        "\n",
        "  tiempo_gpu = datetime.now()\n",
        "\n",
        "  # GPU - Ejecuta el kernel.\n",
        "  # TODO: Falta consultar limites del GPU, para armar las dimensiones correctamente.\n",
        "  dim_hilo = 256\n",
        "  dim_bloque = numpy.int( (cant+dim_hilo-1) / dim_hilo )\n",
        "  print( \"Thread x: \", dim_hilo, \", Bloque x:\", dim_bloque )\n",
        "\n",
        "  #TODO: Ojo, con los tipos de las variables en el kernel.\n",
        "  kernel( numpy.int32(cant), vec1_gpu, vec2_gpu, block=( dim_hilo, 1, 1 ),grid=(dim_bloque, 1,1) )\n",
        "\n",
        "  tiempo_gpu = datetime.now() - tiempo_gpu\n",
        "\n",
        "  # GPU - Copio el resultado desde la memoria GPU.\n",
        "  cuda.memcpy_dtoh( vec2_cpu, vec2_gpu )\n",
        "  \n",
        "  print(\"vector resultado\", vec2_cpu)\n",
        "except:\n",
        "  print(\"Error en la cantidad de elementos, debe ingresar un numero positivo\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vector [ 0 -1  1  0  1]\n",
            "Thread x:  256 , Bloque x: 1\n",
            "vector resultado [ 1  0  1 -1  0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9V5I-JhZGzdM"
      },
      "source": [
        "# **4 Tabla de Pasos**\n",
        "\n",
        " Procesador | Funciòn | Detalle\n",
        "------------|---------|----------\n",
        "CPU      |  @param                | Lectura del tamaño de vectores desde Colab.\n",
        "CPU      |  import                | Importa los módulos para funcionar.\n",
        "CPU      |  datetime.now()        | Toma el tiempo actual.\n",
        "CPU      |  numpy.random.randn( cant ) | Inicializa los vectoes vec1_cpu y vec2_cpu.\n",
        "**GPU**  |  cuda.mem_alloc()      | Reserva la memoria en GPU.\n",
        "**GPU**  |  cuda.memcpy_htod()    | Copia las memorias desde el CPU al GPU.\n",
        "CPU      |  SourceModule()        | Define el código del kernel \n",
        "CPU      |  module.get_function() | Genera la función del kernel GPU\n",
        "CPU      |  dim_tx/dim_bx         | Calcula las dimensiones.\n",
        "**GPU**  |  kernel()              | Ejecuta el kernel en GPU\n",
        "CPU      |  cuda.memcpy_dtoh( )   | Copia el resultado desde GPU memoria vec2_gpu a CPU memoria vec2_cpu.\n",
        "CPU      |  print()               | Informo los resultados.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMyKM1msQO1W"
      },
      "source": [
        "#**5 Conclusion**\n",
        "\n",
        "Con la paralelizacion se puede observar que la complejidad computacional es constante[4], lo cual resulta mas optimo a comparacion de la ejecucion secuencial, la cual habia dado O(n). Por otra parte, esta es una solución simple, no aspira a aplicar paralelismo masivo porque el máximo tamaño de bloque es 256 hilos, con lo que ése sería el mayor vector que este\n",
        "código podría aceptar como entrada.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zf4AOTlwaGVE"
      },
      "source": [
        "#**6 Bibliografia**\n",
        "[1] Invertir un vector: [WEB](https://parzibyte.me/blog/2019/07/18/invertir-arreglo-java/)\n",
        "\n",
        "[2] PyCUDA: [WEB](https://documen.tician.de/pycuda/index.html)\n",
        "\n",
        "[3] Nvidia: [PDF](https://kr.nvidia.com/content/cudazone/download/showcase/kr/Exercises-copy-kernel-launch-reverse.pdf)\n",
        "\n",
        "[4] Complejidad Computacional:[PDF](https://www.frlp.utn.edu.ar/materias/algoritmos/GUIACOMPLEJIDADDEALGORITMOS.pdf)\n"
      ]
    }
  ]
}