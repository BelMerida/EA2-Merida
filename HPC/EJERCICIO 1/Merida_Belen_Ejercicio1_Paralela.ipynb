{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ejercicio1-Paralela.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMP0VLZqkt60"
      },
      "source": [
        "#**1 Introduccion**\n",
        "En el siguiente cuaderno se realiza el codigo para invertir un vector, es decir:\n",
        "<center>$Dado A=(a1,a2,a3,a4,a5)$</center>\n",
        "<center>$El resultado es A´(a5,a4,a3,a2,a1)$</center>\n",
        "\n",
        "Para el cual se pide por parametro la cantidad de elementos para vector a invertir. \n",
        "\n",
        "En cuanto a la paralelizacion[1]:\n",
        "\n",
        "Se define la memoria de los vectores en CPU, se debe reeservar espacio de memoria GPU para los vectores a usar y luego se debe realizar la transferencia de datos de CPU a GPU. Luego, se debe definir la función kernel que ejecutará en GPU, dentro de esta funcion, para acceder al indice de la malla y al indice del hilo dentro del bloque se debe emplear: **blockIdx.x** y **threadIdx.x**;\n",
        "Luego se calcula los índices globales donde cada hilo tenga que trabajar\n",
        "en un área de datos diferente según la partición:\n",
        "**int Idx = blockIdx.x * blockDim.x + threadIdx.x;** \n",
        "Genero la funcion *kernel* y luego lo ejecuto.\n",
        "Para poder mostrar por pantalla el resultado, realizo la transferencia de dados de GPU a CPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8LGHi76mFT9"
      },
      "source": [
        "#**2 Armado del Ambiente**\n",
        "Instala en el cuaderno el modulo CUDA de Python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etJu7Z3OmZXv",
        "outputId": "ffc59f29-b75b-4e34-a6fc-3a6d9aa7b2ee"
      },
      "source": [
        "!pip install pycuda"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pycuda in /usr/local/lib/python3.6/dist-packages (2020.1)\n",
            "Requirement already satisfied: pytools>=2011.2 in /usr/local/lib/python3.6/dist-packages (from pycuda) (2020.4.3)\n",
            "Requirement already satisfied: mako in /usr/local/lib/python3.6/dist-packages (from pycuda) (1.1.3)\n",
            "Requirement already satisfied: decorator>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from pycuda) (4.4.2)\n",
            "Requirement already satisfied: appdirs>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from pycuda) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from pytools>=2011.2->pycuda) (1.18.5)\n",
            "Requirement already satisfied: six>=1.8.0 in /usr/local/lib/python3.6/dist-packages (from pytools>=2011.2->pycuda) (1.15.0)\n",
            "Requirement already satisfied: dataclasses>=0.7; python_version <= \"3.6\" in /usr/local/lib/python3.6/dist-packages (from pytools>=2011.2->pycuda) (0.7)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from mako->pycuda) (1.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1Zst-iAmRBd"
      },
      "source": [
        "#**3 Desarrollo**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdFtkGtBmbO1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b08d254b-c8ae-47f6-a0ed-712d65856d20"
      },
      "source": [
        "# --------------------------------------------\n",
        "#@title 3.1 Parámetros de ejecución { vertical-output: true }\n",
        "\n",
        "cant =   8#@param {type: \"number\"}\n",
        "\n",
        "# --------------------------------------------\n",
        "from datetime import datetime\n",
        "\n",
        "tiempo_total = datetime.now()\n",
        "\n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "from pycuda.compiler import SourceModule\n",
        "\n",
        "import numpy\n",
        "\n",
        "# --------------------------------------------\n",
        "# Definición de función que transforma el tiempo en  milisegundos \n",
        "tiempo_en_ms = lambda dt:(dt.days * 24 * 60 * 60 + dt.seconds) * 1000 + dt.microseconds / 1000.0\n",
        "# --------------------------------------------\n",
        "\n",
        "vec1_cpu = numpy.random.randn( cant )\n",
        "vec1_cpu = vec1_cpu.astype( numpy.int32() )\n",
        "\n",
        "vec2_cpu = numpy.random.randn(cant)\n",
        "vec2_cpu = vec2_cpu.astype(numpy.int32())\n",
        "\n",
        "vec1_gpu = cuda.mem_alloc( vec1_cpu.nbytes )\n",
        "vec2_gpu = cuda.mem_alloc( vec2_cpu.nbytes )\n",
        "\n",
        "cuda.memcpy_htod( vec1_gpu, vec1_cpu )\n",
        "cuda.memcpy_htod( vec2_gpu, vec2_cpu )\n",
        "\n",
        "print(\"vector\", vec1_cpu)\n",
        "\n",
        "module = SourceModule(\"\"\"\n",
        "__global__ void invertirVector( int n, int *V, int *Y)\n",
        "{\n",
        "  int idx = threadIdx.x + blockIdx.x*blockDim.x;\n",
        "  \n",
        "  if( idx <= n ){\n",
        "    Y[idx] = V[n-1-idx];\n",
        "  }\n",
        "}\n",
        "\"\"\") \n",
        "# CPU - Genero la función kernel.\n",
        "kernel = module.get_function(\"invertirVector\")\n",
        "\n",
        "tiempo_gpu = datetime.now()\n",
        "\n",
        "# GPU - Ejecuta el kernel.\n",
        "# TODO: Falta consultar limites del GPU, para armar las dimensiones correctamente.\n",
        "dim_hilo = 256\n",
        "dim_bloque = numpy.int( (cant+dim_hilo-1) / dim_hilo )\n",
        "print( \"Thread x: \", dim_hilo, \", Bloque x:\", dim_bloque )\n",
        "\n",
        "#TODO: Ojo, con los tipos de las variables en el kernel.\n",
        "kernel( numpy.int32(cant), vec1_gpu, vec2_gpu, block=( dim_hilo, 1, 1 ),grid=(dim_bloque, 1,1) )\n",
        "\n",
        "tiempo_gpu = datetime.now() - tiempo_gpu\n",
        "\n",
        "# GPU - Copio el resultado desde la memoria GPU.\n",
        "cuda.memcpy_dtoh( vec2_cpu, vec2_gpu )\n",
        "\n",
        "\n",
        "print(\"vector resultado\", vec2_cpu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vector [ 1  0  0  0 -1  0  0 -1]\n",
            "Thread x:  256 , Bloque x: 1\n",
            "vector resultado [-1  0  0 -1  0  0  0  1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9V5I-JhZGzdM"
      },
      "source": [
        "# **4 Tabla de Pasos**\n",
        "\n",
        " Procesador | Funciòn | Detalle\n",
        "------------|---------|----------\n",
        "CPU      |  @param                | Lectura del tamaño de vectores desde Colab.\n",
        "CPU      |  import                | Importa los módulos para funcionar.\n",
        "CPU      |  datetime.now()        | Toma el tiempo actual.\n",
        "CPU      |  numpy.random.randn( cant ) | Inicializa los vectoes vec1_cpu y vec2_cpu.\n",
        "**GPU**  |  cuda.mem_alloc()      | Reserva la memoria en GPU.\n",
        "**GPU**  |  cuda.memcpy_htod()    | Copia las memorias desde el CPU al GPU.\n",
        "CPU      |  SourceModule()        | Define el código del kernel \n",
        "CPU      |  module.get_function() | Genera la función del kernel GPU\n",
        "CPU      |  dim_tx/dim_bx         | Calcula las dimensiones.\n",
        "**GPU**  |  kernel()              | Ejecuta el kernel en GPU\n",
        "CPU      |  cuda.memcpy_dtoh( )   | Copia el resultado desde GPU memoria vec2_gpu a CPU memoria vec2_cpu.\n",
        "CPU      |  print()               | Informo los resultados.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMyKM1msQO1W"
      },
      "source": [
        "#**5 Conclusion**\n",
        "\n",
        "Con la paralelizacion se puede observar que la complejidad computacional es constante[2], lo cual resulta mas optimo a comparacion de la ejecucion secuencial, la cual habia dado O(n). Por otra parte, esta es una solución simple, no aspira a aplicar paralelismo masivo porque el máximo tamaño de bloque es 256 hilos, con lo que ése sería el mayor vector que este\n",
        "código podría aceptar como entrada.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zf4AOTlwaGVE"
      },
      "source": [
        "#**6 Bibliografia**\n",
        "[1] PyCUDA: [WEB](https://documen.tician.de/pycuda/index.html)\n",
        "\n",
        "[2] Complejidad Computacional:[PDF](https://www.frlp.utn.edu.ar/materias/algoritmos/GUIACOMPLEJIDADDEALGORITMOS.pdf)\n"
      ]
    }
  ]
}